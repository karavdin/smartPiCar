{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karavdin/smartPiCar/blob/main/code/training_tensorflow_lite_LEGO_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQCnYPVDrsgx"
      },
      "source": [
        "# Training a model to detect LEGO objects with Raspberry Pi in Real Time\n",
        "\n",
        "This notebook is based on tutorial and Colab Notebook [\"Training a Raspberry Pi to Detect Traffic Signs and People in Real Time\"](https://github.com/dctian/DeepPiCar/blob/master/models/object_detection/code/tensorflow_traffic_sign_detection.ipynb). The original notebook was written 4 years ago and does not work. Therefore is was updated following methods used in \n",
        "[\"TensorFlow Lite Python object detection example with Raspberry Pi\"](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi) from Tensorflow-Lite team.  \n",
        "\n",
        "As the Raspberry Pi is fairly limited on CPU power and can only run object detection at 1-2 FPS (frames/sec), Google's [EdgeTPU USB Accelarator](https://coral.withgoogle.com/products/accelerator), which can detect objects at 12 FPS, will be used for the model inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk1-8_tshY7_"
      },
      "source": [
        "# Section 0: Check GPU connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHErm8TDhdmj",
        "outputId": "6ce8ce09-bb4a-4d26-8c53-dc7f2dc863c1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr  7 19:48:43 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    47W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNaf5LPHi60k",
        "outputId": "cf78d003-e88c-4d00-ebe3-f02ce2ed39bf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGFLFkv6Bvbc"
      },
      "source": [
        "# Section 1: Install and import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nlZui85hBvbc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q tflite-model-maker\n",
        "!pip install -q tflite-support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "6oX41b7rBvbd",
        "outputId": "67285645-1d17-4884-c1ab-d8ed46821be0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.4 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem ."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "from tflite_support import metadata\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH6FF0-xBvbd"
      },
      "source": [
        "# Section 2: Prepare dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CWgcW__Bvbe"
      },
      "source": [
        "## Clone repository with data and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aoe-AuGvBvbf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# If you forked the repository, you can replace the link.\n",
        "repo_url = 'https://github.com/karavdin/smartPiCar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji4fiTxoBvbg",
        "outputId": "0f23d20f-b5af-46a7-b201-41d1534d2981",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'smartPiCar'...\n",
            "remote: Enumerating objects: 270, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 270 (delta 6), reused 23 (delta 4), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (270/270), 105.12 MiB | 56.36 MiB/s, done.\n",
            "Resolving deltas: 100% (117/117), done.\n",
            "/content/smartPiCar\n",
            "Pull it so that we have the latest code/data\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "%cd /content\n",
        "\n",
        "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd {repo_dir_path}\n",
        "\n",
        "print('Pull it so that we have the latest code/data')\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-wQciusBvbh",
        "outputId": "40e606ae-31ca-4f9b-c151-4bf62affe6f4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully converted xml to csv.\n",
            "Generate `data/annotations/label_map.pbtxt`\n"
          ]
        }
      ],
      "source": [
        "#%cd {repo_dir_path}/models/object_detection\n",
        "\n",
        "# Convert train folder annotation xml files to a single csv file,\n",
        "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
        "!python code/xml_to_csv.py -i data/images/train -o data/annotations/train_labels.csv -l data/annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb5mPmyIBvbi",
        "outputId": "e53f1529-42eb-4b04-f60a-0ab177fda6b8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bike', 'clock', 'Babushka', 'e-bike station', 'Sheila', 'Capitan', 'Bob', 'Phil', 'Nastya', 'Cam', 'ukulele', 'disko', 'Ukulele']\n"
          ]
        }
      ],
      "source": [
        "df_train_labels = pd.read_csv('data/annotations/train_labels.csv')\n",
        "labels = list(df_train_labels['class'].unique())\n",
        "print(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LWUEH8GHBvbi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    'data/images/train',\n",
        "    'data/images/train',\n",
        "    labels\n",
        ")\n",
        "\n",
        "val_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    'data/images/test',\n",
        "    'data/images/test',\n",
        "    labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24xzPFaqDBaS"
      },
      "source": [
        "# Section 3: Select a model architecture\n",
        "\n",
        "EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 146           | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 259           | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 396           | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 716           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 1886          | 41.96%               |\n",
        "\n",
        "<i> * Size of the integer quantized models. <br/>\n",
        "** Latency measured on Raspberry Pi 4 using 4 threads on CPU. <br/>\n",
        "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
        "</i>\n",
        "\n",
        "In this notebook, we use EfficientDet-Lite0 to train our model. You can choose other model architectures depending on whether speed or accuracy is more important to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZbdRYQ7-DNeN",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('efficientdet_lite0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zVPgRPlDVTi"
      },
      "source": [
        "# Section 4: Train the TensorFlow model with the training data.\n",
        "\n",
        "* Set `epochs = 1000`, which means it will go through the training dataset 1000 times. You can look at the validation accuracy during training and stop when you see validation loss (`val_loss`) stop decreasing to avoid overfitting.\n",
        "* Set `batch_size = 5` here so you will see that it takes 20 steps to go through the 100 images in the training dataset.\n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqxc89PaDZ0_",
        "outputId": "a43d7ffb-9394-47fe-fe2a-155ba240e122",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n"
          ]
        }
      ],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=5, train_whole_model=True, epochs=1000, validation_data=val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4aqS4gSDl3G"
      },
      "source": [
        "# Section 5: Evaluate the model with the validation data.\n",
        "\n",
        "After training the object detection model using the images in the training dataset, use the 10 images in the validation dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 1 step to go through the 10 images in the validation dataset.\n",
        "\n",
        "The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZSOoESdDo1u",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model.evaluate(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5wljd0aDxjk"
      },
      "source": [
        "# Section 6: Export as a TensorFlow Lite model.\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is [full integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant). This allows the TensorFlow Lite model to be smaller, run faster on Raspberry Pi CPU and also compatible with the Google Coral EdgeTPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiijzHHhD8Gs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', tflite_filename='android.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg90emK1ECMe"
      },
      "source": [
        "# Section 7:  Evaluate the TensorFlow Lite model.\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOVxiPk_EO-w",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model.evaluate_tflite('android.tflite', val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OnXkNFuFNsr"
      },
      "source": [
        "# Section 9: Visual validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un6msowWFQ_H",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title Load the trained TFLite model and define some visualization functions\n",
        "\n",
        "#@markdown This code comes from the TFLite Object Detection [Raspberry Pi sample](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi).\n",
        "\n",
        "import platform\n",
        "from typing import List, NamedTuple\n",
        "import json\n",
        "\n",
        "import cv2\n",
        "\n",
        "Interpreter = tf.lite.Interpreter\n",
        "load_delegate = tf.lite.experimental.load_delegate\n",
        "\n",
        "# pylint: enable=g-import-not-at-top\n",
        "\n",
        "\n",
        "class ObjectDetectorOptions(NamedTuple):\n",
        "  \"\"\"A config to initialize an object detector.\"\"\"\n",
        "\n",
        "  enable_edgetpu: bool = False\n",
        "  \"\"\"Enable the model to run on EdgeTPU.\"\"\"\n",
        "\n",
        "  label_allow_list: List[str] = None\n",
        "  \"\"\"The optional allow list of labels.\"\"\"\n",
        "\n",
        "  label_deny_list: List[str] = None\n",
        "  \"\"\"The optional deny list of labels.\"\"\"\n",
        "\n",
        "  max_results: int = -1\n",
        "  \"\"\"The maximum number of top-scored detection results to return.\"\"\"\n",
        "\n",
        "  num_threads: int = 1\n",
        "  \"\"\"The number of CPU threads to be used.\"\"\"\n",
        "\n",
        "  score_threshold: float = 0.0\n",
        "  \"\"\"The score threshold of detection results to return.\"\"\"\n",
        "\n",
        "\n",
        "class Rect(NamedTuple):\n",
        "  \"\"\"A rectangle in 2D space.\"\"\"\n",
        "  left: float\n",
        "  top: float\n",
        "  right: float\n",
        "  bottom: float\n",
        "\n",
        "\n",
        "class Category(NamedTuple):\n",
        "  \"\"\"A result of a classification task.\"\"\"\n",
        "  label: str\n",
        "  score: float\n",
        "  index: int\n",
        "\n",
        "\n",
        "class Detection(NamedTuple):\n",
        "  \"\"\"A detected object as the result of an ObjectDetector.\"\"\"\n",
        "  bounding_box: Rect\n",
        "  categories: List[Category]\n",
        "\n",
        "\n",
        "def edgetpu_lib_name():\n",
        "  \"\"\"Returns the library name of EdgeTPU in the current platform.\"\"\"\n",
        "  return {\n",
        "      'Darwin': 'libedgetpu.1.dylib',\n",
        "      'Linux': 'libedgetpu.so.1',\n",
        "      'Windows': 'edgetpu.dll',\n",
        "  }.get(platform.system(), None)\n",
        "\n",
        "\n",
        "class ObjectDetector:\n",
        "  \"\"\"A wrapper class for a TFLite object detection model.\"\"\"\n",
        "\n",
        "  _OUTPUT_LOCATION_NAME = 'location'\n",
        "  _OUTPUT_CATEGORY_NAME = 'category'\n",
        "  _OUTPUT_SCORE_NAME = 'score'\n",
        "  _OUTPUT_NUMBER_NAME = 'number of detections'\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      model_path: str,\n",
        "      options: ObjectDetectorOptions = ObjectDetectorOptions()\n",
        "  ) -> None:\n",
        "    \"\"\"Initialize a TFLite object detection model.\n",
        "    Args:\n",
        "        model_path: Path to the TFLite model.\n",
        "        options: The config to initialize an object detector. (Optional)\n",
        "    Raises:\n",
        "        ValueError: If the TFLite model is invalid.\n",
        "        OSError: If the current OS isn't supported by EdgeTPU.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load metadata from model.\n",
        "    displayer = metadata.MetadataDisplayer.with_model_file(model_path)\n",
        "\n",
        "    # Save model metadata for preprocessing later.\n",
        "    model_metadata = json.loads(displayer.get_metadata_json())\n",
        "    process_units = model_metadata['subgraph_metadata'][0]['input_tensor_metadata'][0]['process_units']\n",
        "    mean = 0.0\n",
        "    std = 1.0\n",
        "    for option in process_units:\n",
        "      if option['options_type'] == 'NormalizationOptions':\n",
        "        mean = option['options']['mean'][0]\n",
        "        std = option['options']['std'][0]\n",
        "    self._mean = mean\n",
        "    self._std = std\n",
        "\n",
        "    # Load label list from metadata.\n",
        "    file_name = displayer.get_packed_associated_file_list()[0]\n",
        "    label_map_file = displayer.get_associated_file_buffer(file_name).decode()\n",
        "    label_list = list(filter(lambda x: len(x) > 0, label_map_file.splitlines()))\n",
        "    self._label_list = label_list\n",
        "\n",
        "    # Initialize TFLite model.\n",
        "    if options.enable_edgetpu:\n",
        "      if edgetpu_lib_name() is None:\n",
        "        raise OSError(\"The current OS isn't supported by Coral EdgeTPU.\")\n",
        "      interpreter = Interpreter(\n",
        "          model_path=model_path,\n",
        "          experimental_delegates=[load_delegate(edgetpu_lib_name())],\n",
        "          num_threads=options.num_threads)\n",
        "    else:\n",
        "      interpreter = Interpreter(\n",
        "          model_path=model_path, num_threads=options.num_threads)\n",
        "\n",
        "    interpreter.allocate_tensors()\n",
        "    input_detail = interpreter.get_input_details()[0]\n",
        "\n",
        "    # From TensorFlow 2.6, the order of the outputs become undefined.\n",
        "    # Therefore we need to sort the tensor indices of TFLite outputs and to know\n",
        "    # exactly the meaning of each output tensor. For example, if\n",
        "    # output indices are [601, 599, 598, 600], tensor names and indices aligned\n",
        "    # are:\n",
        "    #   - location: 598\n",
        "    #   - category: 599\n",
        "    #   - score: 600\n",
        "    #   - detection_count: 601\n",
        "    # because of the op's ports of TFLITE_DETECTION_POST_PROCESS\n",
        "    # (https://github.com/tensorflow/tensorflow/blob/a4fe268ea084e7d323133ed7b986e0ae259a2bc7/tensorflow/lite/kernels/detection_postprocess.cc#L47-L50).\n",
        "    sorted_output_indices = sorted(\n",
        "        [output['index'] for output in interpreter.get_output_details()])\n",
        "    self._output_indices = {\n",
        "        self._OUTPUT_LOCATION_NAME: sorted_output_indices[0],\n",
        "        self._OUTPUT_CATEGORY_NAME: sorted_output_indices[1],\n",
        "        self._OUTPUT_SCORE_NAME: sorted_output_indices[2],\n",
        "        self._OUTPUT_NUMBER_NAME: sorted_output_indices[3],\n",
        "    }\n",
        "\n",
        "    self._input_size = input_detail['shape'][2], input_detail['shape'][1]\n",
        "    self._is_quantized_input = input_detail['dtype'] == np.uint8\n",
        "    self._interpreter = interpreter\n",
        "    self._options = options\n",
        "\n",
        "  def detect(self, input_image: np.ndarray) -> List[Detection]:\n",
        "    \"\"\"Run detection on an input image.\n",
        "    Args:\n",
        "        input_image: A [height, width, 3] RGB image. Note that height and width\n",
        "          can be anything since the image will be immediately resized according\n",
        "          to the needs of the model within this function.\n",
        "    Returns:\n",
        "        A Person instance.\n",
        "    \"\"\"\n",
        "    image_height, image_width, _ = input_image.shape\n",
        "\n",
        "    input_tensor = self._preprocess(input_image)\n",
        "\n",
        "    self._set_input_tensor(input_tensor)\n",
        "    self._interpreter.invoke()\n",
        "\n",
        "    # Get all output details\n",
        "    boxes = self._get_output_tensor(self._OUTPUT_LOCATION_NAME)\n",
        "    classes = self._get_output_tensor(self._OUTPUT_CATEGORY_NAME)\n",
        "    scores = self._get_output_tensor(self._OUTPUT_SCORE_NAME)\n",
        "    count = int(self._get_output_tensor(self._OUTPUT_NUMBER_NAME))\n",
        "\n",
        "    return self._postprocess(boxes, classes, scores, count, image_width,\n",
        "                             image_height)\n",
        "\n",
        "  def _preprocess(self, input_image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Preprocess the input image as required by the TFLite model.\"\"\"\n",
        "\n",
        "    # Resize the input\n",
        "    input_tensor = cv2.resize(input_image, self._input_size)\n",
        "\n",
        "    # Normalize the input if it's a float model (aka. not quantized)\n",
        "    if not self._is_quantized_input:\n",
        "      input_tensor = (np.float32(input_tensor) - self._mean) / self._std\n",
        "\n",
        "    # Add batch dimension\n",
        "    input_tensor = np.expand_dims(input_tensor, axis=0)\n",
        "\n",
        "    return input_tensor\n",
        "\n",
        "  def _set_input_tensor(self, image):\n",
        "    \"\"\"Sets the input tensor.\"\"\"\n",
        "    tensor_index = self._interpreter.get_input_details()[0]['index']\n",
        "    input_tensor = self._interpreter.tensor(tensor_index)()[0]\n",
        "    input_tensor[:, :] = image\n",
        "\n",
        "  def _get_output_tensor(self, name):\n",
        "    \"\"\"Returns the output tensor at the given index.\"\"\"\n",
        "    output_index = self._output_indices[name]\n",
        "    tensor = np.squeeze(self._interpreter.get_tensor(output_index))\n",
        "    return tensor\n",
        "\n",
        "  def _postprocess(self, boxes: np.ndarray, classes: np.ndarray,\n",
        "                   scores: np.ndarray, count: int, image_width: int,\n",
        "                   image_height: int) -> List[Detection]:\n",
        "    \"\"\"Post-process the output of TFLite model into a list of Detection objects.\n",
        "    Args:\n",
        "        boxes: Bounding boxes of detected objects from the TFLite model.\n",
        "        classes: Class index of the detected objects from the TFLite model.\n",
        "        scores: Confidence scores of the detected objects from the TFLite model.\n",
        "        count: Number of detected objects from the TFLite model.\n",
        "        image_width: Width of the input image.\n",
        "        image_height: Height of the input image.\n",
        "    Returns:\n",
        "        A list of Detection objects detected by the TFLite model.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Parse the model output into a list of Detection entities.\n",
        "    for i in range(count):\n",
        "      if scores[i] >= self._options.score_threshold:\n",
        "        y_min, x_min, y_max, x_max = boxes[i]\n",
        "        bounding_box = Rect(\n",
        "            top=int(y_min * image_height),\n",
        "            left=int(x_min * image_width),\n",
        "            bottom=int(y_max * image_height),\n",
        "            right=int(x_max * image_width))\n",
        "        class_id = int(classes[i])\n",
        "        category = Category(\n",
        "            score=scores[i],\n",
        "            label=self._label_list[class_id],  # 0 is reserved for background\n",
        "            index=class_id)\n",
        "        result = Detection(bounding_box=bounding_box, categories=[category])\n",
        "        results.append(result)\n",
        "\n",
        "    # Sort detection results by score ascending\n",
        "    sorted_results = sorted(\n",
        "        results,\n",
        "        key=lambda detection: detection.categories[0].score,\n",
        "        reverse=True)\n",
        "\n",
        "    # Filter out detections in deny list\n",
        "    filtered_results = sorted_results\n",
        "    if self._options.label_deny_list is not None:\n",
        "      filtered_results = list(\n",
        "          filter(\n",
        "              lambda detection: detection.categories[0].label not in self.\n",
        "              _options.label_deny_list, filtered_results))\n",
        "\n",
        "    # Keep only detections in allow list\n",
        "    if self._options.label_allow_list is not None:\n",
        "      filtered_results = list(\n",
        "          filter(\n",
        "              lambda detection: detection.categories[0].label in self._options.\n",
        "              label_allow_list, filtered_results))\n",
        "\n",
        "    # Only return maximum of max_results detection.\n",
        "    if self._options.max_results > 0:\n",
        "      result_count = min(len(filtered_results), self._options.max_results)\n",
        "      filtered_results = filtered_results[:result_count]\n",
        "\n",
        "    return filtered_results\n",
        "\n",
        "\n",
        "_MARGIN = 10  # pixels\n",
        "_ROW_SIZE = 10  # pixels\n",
        "_FONT_SIZE = 1\n",
        "_FONT_THICKNESS = 1\n",
        "_TEXT_COLOR = (0, 0, 255)  # red\n",
        "\n",
        "\n",
        "def visualize(\n",
        "    image: np.ndarray,\n",
        "    detections: List[Detection],\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Draws bounding boxes on the input image and return it.\n",
        "  Args:\n",
        "    image: The input RGB image.\n",
        "    detections: The list of all \"Detection\" entities to be visualize.\n",
        "  Returns:\n",
        "    Image with bounding boxes.\n",
        "  \"\"\"\n",
        "  for detection in detections:\n",
        "    # Draw bounding_box\n",
        "    start_point = detection.bounding_box.left, detection.bounding_box.top\n",
        "    end_point = detection.bounding_box.right, detection.bounding_box.bottom\n",
        "    cv2.rectangle(image, start_point, end_point, _TEXT_COLOR, 3)\n",
        "\n",
        "    # Draw label and score\n",
        "    category = detection.categories[0]\n",
        "    class_name = category.label\n",
        "    probability = round(category.score, 2)\n",
        "    result_text = class_name + ' (' + str(probability) + ')'\n",
        "    text_location = (_MARGIN + detection.bounding_box.left,\n",
        "                     _MARGIN + _ROW_SIZE + detection.bounding_box.top)\n",
        "    cv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
        "                _FONT_SIZE, _TEXT_COLOR, _FONT_THICKNESS)\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyIBDKwJIfZY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "PATH_TO_TEST_IMAGES_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW_CclH1IimE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!ls -l /content/smartPiCar/data/images/test/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJSdc2N6FqAA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title Run object detection and show the detection results\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "#INPUT_IMAGE_URL = \"http://download.tensorflow.org/example_images/android_figurine.jpg\" #@param {type:\"string\"}\n",
        "DETECTION_THRESHOLD = 0.25 #@param {type:\"number\"}\n",
        "TFLITE_MODEL_PATH = \"android.tflite\" #@param {type:\"string\"}\n",
        "\n",
        "#TEMP_FILE = '/tmp/image.png'\n",
        "\n",
        "#!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL\n",
        "\n",
        "PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, \"data/images/test/\")\n",
        "\n",
        "# directory = os.fsencode(PATH_TO_TEST_IMAGES_DIR)\n",
        "# for input_file in os.listdir(directory):\n",
        "#   filename = os.fsdecode(input_file)\n",
        "#   #print(filename)\n",
        "#   #if filename.endswith(\".jpeg\") or filename.endswith(\".jpg\"):\n",
        "#   if filename == \"test_6.jpg\":\n",
        "#     print('Running inference for {}... '.format(filename), end='')\n",
        "\n",
        "image = Image.open(PATH_TO_TEST_IMAGES_DIR+\"test_90.jpg\").convert('RGB')\n",
        "image.thumbnail((512, 512), Image.ANTIALIAS)\n",
        "image_np = np.asarray(image)\n",
        "\n",
        "# Load the TFLite model\n",
        "options = ObjectDetectorOptions(\n",
        "    num_threads=4,\n",
        "    score_threshold=DETECTION_THRESHOLD,)\n",
        "detector = ObjectDetector(model_path=TFLITE_MODEL_PATH, options=options)\n",
        "# Run object detection estimation using the model.\n",
        "detections = detector.detect(image_np)\n",
        "\n",
        "# Draw keypoints and edges on input image\n",
        "image_np = visualize(image_np, detections)\n",
        "\n",
        "# Show the detection result\n",
        "Image.fromarray(image_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEPl01wMEXyX"
      },
      "source": [
        "# Section 10: Download the TFLite model to your local computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC87EwwlET_k",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('android.tflite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIVAassf8dka",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "--- OLD CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etOThFKokSmU"
      },
      "source": [
        "# Section 1: Mount Google drive\n",
        "Mount my Google Drive and save modeling output files (`.ckpt`)  there, so that it won't be wiped out when colab Virtual Machine restarts.  It has an idle timeout of 90 min, and maximum daily usage of 12 hours.\n",
        "\n",
        "Google will ask for an authenticate code when you run the following code, just follow the link in the output and allow access.   You can put the `model_dir` anywhere in your google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhanHaj771t-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "model_dir = '/content/gdrive/My Drive/Colab Notebooks/DeepCar_Training'\n",
        "# !rm -rf '{model_dir}'\n",
        "# os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "#!ls -ltra '{model_dir}'/..\n",
        "!ls -ltra '{model_dir}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhzxsJb3dpWq"
      },
      "source": [
        "# Section 2: Configs and Hyperparameters\n",
        "\n",
        "Support a variety of models, you can find more pretrained model from [Tensorflow detection model zoo: COCO-trained models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models), as well as their pipline config files in [object_detection/samples/configs/](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEdxvA7NrRXO"
      },
      "source": [
        "# Not all models are suitable for transfer learning, see: \n",
        "https://github.com/tensorflow/models/issues/9287"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnNXNQCjdniL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# If you forked the repository, you can replace the link.\n",
        "repo_url = 'https://github.com/karavdin/DeepPiCar'\n",
        "\n",
        "# Number of training steps.\n",
        "#num_steps = 100000  # 200000\n",
        "num_steps = 5000  # 200000\n",
        "\n",
        "# Number of evaluation steps.\n",
        "num_eval_steps = 50\n",
        "#num_eval_steps = 5\n",
        "\n",
        "\n",
        "# model configs are from Model Zoo github: \n",
        "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models\n",
        "MODELS_CONFIG = {\n",
        "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz\n",
        "    'ssd_mobilenet_v1_quantized': {\n",
        "        'model_name': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18',\n",
        "        'pipeline_file': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync.config',\n",
        "        'batch_size': 12\n",
        "    },    \n",
        "    # 'ssd_mobilenet_v2': {\n",
        "    #     'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "    #     'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
        "    #     'batch_size': 12\n",
        "    # },\n",
        "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\n",
        "    'ssd_mobilenet_v2_quantized': {\n",
        "        'model_name': 'ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_quantized_300x300_coco.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'faster_rcnn_inception_v2': {\n",
        "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
        "        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'rfcn_resnet101': {\n",
        "        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n",
        "        'pipeline_file': 'rfcn_resnet101_pets.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    #TF2\n",
        "    #http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
        "    'ssd_mobilenet_v2_tf2': { #VERY BAD results after 20k training\n",
        "        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
        "        #'pipeline_file': 'ssd_mobilenet_v3_small.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'ssd_resnet152_v1_tf2': { #Bad results, seems to be issue with saving model\n",
        "        'model_name': 'ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8',\n",
        "        'batch_size': 4\n",
        "    },\n",
        "    'ssd_mobilenet_v2_640_tf2': { \n",
        "        'model_name': 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8',\n",
        "        #'pipeline_file': 'ssd_mobilenet_v3_small.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    \n",
        "}\n",
        "\n",
        "# Pick the model you want to use\n",
        "# Select a model in `MODELS_CONFIG`.\n",
        "# Note: for Edge TPU, you have to:\n",
        "# 1) start with a pretrained model from model zoo, such as above 4\n",
        "# 2) Must be a quantized model, which reduces the model size significantly\n",
        "#selected_model = 'ssd_mobilenet_v2_quantized'\n",
        "#selected_model = 'ssd_mobilenet_v2_tf2'\n",
        "#selected_model = 'ssd_resnet152_v1_tf2'\n",
        "selected_model = 'ssd_mobilenet_v2_640_tf2'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "# Name of the pipline file in tensorflow object detection API.\n",
        "#pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n",
        "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw-YqZHUKv-Y"
      },
      "source": [
        "# Section 3: Set up Training Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_xVkoa4KjHO"
      },
      "source": [
        "## Clone the `DeepPiCar` repository or your fork."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxc3DmvLQF3z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "%cd /content\n",
        "\n",
        "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd {repo_dir_path}\n",
        "\n",
        "print('Pull it so that we have the latest code/data')\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI8__uNS8-ns"
      },
      "source": [
        "## Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBTP_y94F0hH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "#!pip uninstall tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGiQGSg5ByEK",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "# !pip install tf_slim\n",
        "# !pip install tensorflow_io\n",
        "# !pip install tensorflow-addons\n",
        "!pip install tensorflow==2.11.0\n",
        "#!pip install --ignore-installed --upgrade tensorflow==1.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlUNke4GDMYI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"--- Tensorflow version --- \\n\", tf.__version__)\n",
        "#print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF_sywiiippI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecpHEnka8Kix",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q cython contextlib2 pillow lxml matplotlib\n",
        "#!pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI\n",
        "\n",
        "\n",
        "# !git clone https://github.com/cocodataset/cocoapi.git\n",
        "# %cd cocoapi/PythonAPI\n",
        "# !make\n",
        "# !cp -r pycocotools /content/models/research/\n",
        "\n",
        "!pip install -q pycocotools\n",
        "\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install --use-feature=2020-resolver .\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/:/content/models/'\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/models\")\n",
        "\n",
        "#!python object_detection/builders/model_builder_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW8sd8Y_B3W7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!python object_detection/builders/model_builder_tf2_test.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-k7uGThXlny"
      },
      "source": [
        "## Prepare `tfrecord` files\n",
        "\n",
        "Use the following scripts to generate the `tfrecord` files.\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezGDABRXXhPP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%cd {repo_dir_path}/models/object_detection\n",
        "\n",
        "# Convert train folder annotation xml files to a single csv file,\n",
        "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
        "!python code/xml_to_csv.py -i data/images_hh/train -o data/annotations_hh/train_labels.csv -l data/annotations_hh\n",
        "\n",
        "# Convert test folder annotation xml files to a single csv.\n",
        "!python code/xml_to_csv.py -i data/images_hh/test -o data/annotations_hh/test_labels.csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5yq_Xv7VOhy",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtue93YVP3vP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df_train_labels = pd.read_csv('data/annotations_hh/train_labels.csv')\n",
        "print(df_train_labels['class'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MLvAaiWRo1-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df_test_labels = pd.read_csv('data/annotations_hh/test_labels.csv')\n",
        "print(df_test_labels['class'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiWqYfQ7P6S8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Generate `train.record`\n",
        "!python code/generate_tfrecord.py --csv_input=data/annotations_hh/train_labels.csv --output_path=data/annotations_hh/train.record --img_path=data/images_hh/train --label_map data/annotations_hh/label_map.pbtxt\n",
        "\n",
        "# Generate `test.record`\n",
        "!python code/generate_tfrecord.py --csv_input=data/annotations_hh/test_labels.csv --output_path=data/annotations_hh/test.record --img_path=data/images_hh/test --label_map data/annotations_hh/label_map.pbtxt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgd-fzAIkZlV",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "test_record_fname = repo_dir_path + '/models/object_detection/data/annotations_hh/test.record'\n",
        "train_record_fname = repo_dir_path + '/models/object_detection/data/annotations_hh/train.record'\n",
        "label_map_pbtxt_fname = repo_dir_path + '/models/object_detection/data/annotations_hh/label_map.pbtxt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ8otnfyCYdG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#!cat data/annotations_hh/test_labels.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCNYAaC7w6N8"
      },
      "source": [
        "## Download base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orDCj6ihgUMR",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%cd /content/models/research\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "#DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\n",
        "DEST_DIR = '/content/models/research/pretrained_model_2'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)\n",
        "print(MODEL,\"from\",MODEL_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGhvAObeiIix",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!echo {DEST_DIR}\n",
        "!ls -alh {DEST_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHnxlfRznPP3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "fine_tune_checkpoint = os.path.join(DEST_DIR+\"/checkpoint\", \"ckpt-0\")\n",
        "fine_tune_checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYW8J5JoLP4I"
      },
      "source": [
        "# Section 4: Transfer Learning Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvwtHlLOeRJD"
      },
      "source": [
        "## Configuring a Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIhw7IdpLuiU",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "pipeline_fname = os.path.join(DEST_DIR, 'pipeline.config')\n",
        "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG1nCNpUXcRU",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjtCbLF2i0wI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# training pipeline file defines:\n",
        "# - pretrain model path\n",
        "# - the train/test sets\n",
        "# - ID to Label mapping and number of classes\n",
        "# - training batch size\n",
        "# - epochs to trains\n",
        "# - learning rate\n",
        "# - etc\n",
        "\n",
        "# note we just need to use a sample one, and make edits to it.\n",
        "\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open(pipeline_fname, 'w') as f:\n",
        "    \n",
        "    s = s = re.sub(\n",
        "        '(fine_tune_checkpoint_type: \".*?)(classification)(.*?\")', 'fine_tune_checkpoint_type: \"{}\"'.format(\"detection\"), s, 1)\n",
        "\n",
        "    # fine_tune_checkpoint: downloaded pre-trained model checkpoint path\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test, we created earlier with our training/test sets\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s, 1)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s, 1)\n",
        "\n",
        "    # label_map_path: ID to label file\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps (Number of epochs to train)\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "  \n",
        "    f.write(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IH96bbydOWn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#!cat {label_map_pbtxt_fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH0MEEanocn6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# look for num_classes: 13, since we have 13 different objects \n",
        "!cat {pipeline_fname}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23TECXvNezIF"
      },
      "source": [
        "## Run Tensorboard(Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYg_1-3-4ivh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '{model_dir}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaBLawdCThmj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!ls /content/gdrive/My\\ Drive/Colab\\ Notebooks/DeepCar_Training/fine_tuned_model/checkpoint/checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6aUqJDxR5D0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# #%load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "# import tensorflow as tf\n",
        "# import datetime, os\n",
        "\n",
        "# logs_base_dir = \"./logs\"\n",
        "# os.makedirs(logs_base_dir, exist_ok=True)\n",
        "# #%tensorboard --logdir {model_dir}\n",
        "# %tensorboard --logdir=f'{model_dir}/fine_tuned_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of8TO_3iUo7q",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H2PZs-mSCmO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "# !unzip -o ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8o6r1o5SC5M",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# LOG_DIR = model_dir\n",
        "# get_ipython().system_raw(\n",
        "#     'tensorboard --logdir \"{}\" --host 0.0.0.0 --port 6006 &'\n",
        "#     .format(LOG_DIR)\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge1OX7gcSC7S",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#get_ipython().system_raw('./ngrok http 6006 &')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5GSGxZNh8rp"
      },
      "source": [
        "### Get Tensorboard link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjhPT9iPSJ6T",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDddx2rPfex9"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZc0RFcUjTa-"
      },
      "source": [
        "Now all inputs are set up, just train the model.   This process may take a few hours.   Since we are saving the model training results (model.ckpt-* files) in our google drive (a persistent storage that will survice the restart of our colab VM instance), we can safely leave and return a few hours later. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfPUc3YutA4f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install lvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjDHjhKQofT5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# --- TRAINING ---\n",
        "#num_steps = 20\n",
        "#SendEmail(\"Colab train started\")\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --model_dir='{model_dir}' \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --num_eval_steps={num_eval_steps}\n",
        "#SendEmail(\"Colab train finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KP-tUdtnRybs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!ls -ltra '{model_dir}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Kzh3_JLVW-"
      },
      "source": [
        "# Section 5: Save and Convert Model Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmSESMetj1sa"
      },
      "source": [
        "## Exporting a Trained Inference Graph\n",
        "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHoP90pUyKSq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "output_directory = '%s/fine_tuned_model' % model_dir\n",
        "os.makedirs(output_directory, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qDv-MB757l6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!echo exporting the model in other way\n",
        "!python /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "--pipeline_config_path={pipeline_fname} \\\n",
        "--trained_checkpoint_dir='{model_dir}' \\\n",
        "--output_directory='{output_directory}' \\\n",
        "--input_type=image_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1gX19GlVW7"
      },
      "source": [
        "## Run inference test\n",
        "Test with images in repository `object_detection/data/images/test` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOSEPybVPFDz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "\n",
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, \"models/object_detection/data/images_hh/test/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCw7gSwaItyr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "      path: the file path to the image\n",
        "\n",
        "    Returns:\n",
        "      numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    \n",
        "    return np.array(Image.open(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC3AS7qnI0G3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def inference_with_plot(path2images, box_th=0.7):\n",
        "    \"\"\"\n",
        "    Function that performs inference and plots resulting b-boxes\n",
        "    \n",
        "    Args:\n",
        "      path2images: an array with pathes to images\n",
        "      box_th: (float) value that defines threshold for model prediction.\n",
        "      \n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "\n",
        "    directory = os.fsencode(path2images)\n",
        "    for input_file in os.listdir(directory):\n",
        "      filename = os.fsdecode(input_file)\n",
        "      if filename.endswith(\".jpeg\") or filename.endswith(\".jpg\"):\n",
        "      #if filename == \"test_6.jpg\":\n",
        "        print('Running inference for {}... '.format(filename), end='')\n",
        "        #run_detector(detector, path2images+\"/\"+filename)\n",
        "\n",
        "    # for image_path in path2images:\n",
        "\n",
        "    #     print('Running inference for {}... '.format(image_path), end='')\n",
        "\n",
        "        image_np = load_image_into_numpy_array(PATH_TO_TEST_IMAGES_DIR+'/'+filename)\n",
        "        #image_np = load_image_into_numpy_array(PATH_TO_TEST_IMAGES_DIR+'/'+filename)\n",
        "        \n",
        "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "        detections = detect_fn(input_tensor)\n",
        "\n",
        "        # All outputs are batches tensors.\n",
        "        # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "        # We're only interested in the first num_detections.\n",
        "        num_detections = int(detections.pop('num_detections'))\n",
        "        detections = {key: value[0, :num_detections].numpy()\n",
        "                      for key, value in detections.items()}\n",
        "        \n",
        "        detections['num_detections'] = num_detections\n",
        "\n",
        "        # detection_classes should be ints.\n",
        "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "        #print(\"detections['detection_classes']\",detections['detection_classes'])\n",
        "        #print(\"detections['detection_scores']\",detections['detection_scores'])\n",
        "        label_id_offset = 1\n",
        "        image_np_with_detections = image_np.copy()\n",
        "\n",
        "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "                image_np_with_detections,\n",
        "                detections['detection_boxes'],\n",
        "                detections['detection_classes']+label_id_offset,\n",
        "                detections['detection_scores'],\n",
        "                category_index,\n",
        "                use_normalized_coordinates=True,\n",
        "                max_boxes_to_draw=7,\n",
        "                min_score_thresh=box_th,\n",
        "                agnostic_mode=False,\n",
        "                line_thickness=4)\n",
        "        # font = ImageFont.truetype('arial.ttf', 30)\n",
        "        plt.figure(figsize=(15,10))\n",
        "        plt.imshow(image_np_with_detections)\n",
        "        plt.show()\n",
        "        print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqOS9aSQI79K",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def nms(rects, thd=0.5):\n",
        "    \"\"\"\n",
        "    Filter rectangles\n",
        "    rects is array of oblects ([x1,y1,x2,y2], confidence, class)\n",
        "    thd - intersection threshold (intersection divides min square of rectange)\n",
        "    \"\"\"\n",
        "    out = []\n",
        "\n",
        "    remove = [False] * len(rects)\n",
        "\n",
        "    for i in range(0, len(rects) - 1):\n",
        "        if remove[i]:\n",
        "            continue\n",
        "        inter = [0.0] * len(rects)\n",
        "        for j in range(i, len(rects)):\n",
        "            if remove[j]:\n",
        "                continue\n",
        "            inter[j] = intersection(rects[i][0], rects[j][0]) / min(square(rects[i][0]), square(rects[j][0]))\n",
        "\n",
        "        max_prob = 0.0\n",
        "        max_idx = 0\n",
        "        for k in range(i, len(rects)):\n",
        "            if inter[k] >= thd:\n",
        "                if rects[k][1] > max_prob:\n",
        "                    max_prob = rects[k][1]\n",
        "                    max_idx = k\n",
        "\n",
        "        for k in range(i, len(rects)):\n",
        "            if (inter[k] >= thd) & (k != max_idx):\n",
        "                remove[k] = True\n",
        "\n",
        "    for k in range(0, len(rects)):\n",
        "        if not remove[k]:\n",
        "            out.append(rects[k])\n",
        "\n",
        "    boxes = [box[0] for box in out]\n",
        "    scores = [score[1] for score in out]\n",
        "    classes = [cls[2] for cls in out]\n",
        "    return boxes, scores, classes\n",
        "\n",
        "\n",
        "def intersection(rect1, rect2):\n",
        "    \"\"\"\n",
        "    Calculates square of intersection of two rectangles\n",
        "    rect: list with coords of top-right and left-boom corners [x1,y1,x2,y2]\n",
        "    return: square of intersection\n",
        "    \"\"\"\n",
        "    x_overlap = max(0, min(rect1[2], rect2[2]) - max(rect1[0], rect2[0]));\n",
        "    y_overlap = max(0, min(rect1[3], rect2[3]) - max(rect1[1], rect2[1]));\n",
        "    overlapArea = x_overlap * y_overlap;\n",
        "    return overlapArea\n",
        "\n",
        "\n",
        "def square(rect):\n",
        "    \"\"\"\n",
        "    Calculates square of rectangle\n",
        "    \"\"\"\n",
        "    return abs(rect[2] - rect[0]) * abs(rect[3] - rect[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQW-d5hcLrjG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# do not change anything in this cell\n",
        "# importing all scripts that will be needed to export your model and use it for inference\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "configs = config_util.get_configs_from_pipeline_file(f'{output_directory}/pipeline.config') # importing config\n",
        "model_config = configs['model'] # recreating model config\n",
        "detection_model = model_builder.build(model_config=model_config, is_training=False) # importing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvPLvlt3PfWW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "lst = os.listdir(model_dir)\n",
        "# find the last model checkpoint file, i.e. model.ckpt-1000.meta\n",
        "lst = [l for l in lst if 'ckpt-' in l and '.index' in l]\n",
        "#print(lst)\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.index', '')\n",
        "#last_model = lst[steps.argmax()]\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "print(last_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SAMpfaJPPId",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "ckpt.restore(last_model_path).expect_partial()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SnVXoPDP_YE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def detect_fn(image):\n",
        "    \"\"\"\n",
        "    Detect objects in image.\n",
        "    \n",
        "    Args:\n",
        "      image: (tf.tensor): 4D input image\n",
        "      \n",
        "    Returs:\n",
        "      detections (dict): predictions that model made\n",
        "    \"\"\"\n",
        "\n",
        "    image, shapes = detection_model.preprocess(image)\n",
        "    prediction_dict = detection_model.predict(image, shapes)\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "    #print(\"detections:\",detections)\n",
        "    return detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l76_ZSY29Jbw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbFI7KUKzk9t",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96Q4yCHLGoo7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#!cat object_detection/utils/visualization_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O30iqhE_Mm5L",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2YwPrmKBw64",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Different font-type and font-size for labels text\n",
        "!wget https://freefontsdownload.net/download/160187/arial.zip\n",
        "!unzip arial.zip -d .\n",
        "!sed -i \"s/font = ImageFont.truetype('arial.ttf', 24)/font = ImageFont.truetype('arial.ttf', 102)/\" object_detection/utils/visualization_utils.py\n",
        "#!sed -i \"s/font = ImageFont.truetype('arial.ttf', 92)/font = ImageFont.truetype('arial.ttf', 122)/\" object_detection/utils/visualization_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USL1jX47BsfJ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#PATH_TO_TEST_IMAGES_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmf3rwVTKPnI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "inference_with_plot(PATH_TO_TEST_IMAGES_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvVglsy1Bwjh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!ls /content/DeepPiCar/models/object_detection/data/images_hh/test/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apt1IjG3EClk"
      },
      "source": [
        "# Store model as tf-light object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7K3_tE45Wrd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!echo creates the frozen inference graph in fine_tune_model\n",
        "# there is an \"Incomplete shape\" message.  but we can safely ignore that. \n",
        "!python /content/models/research/object_detection/export_tflite_graph_lib_tf2.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory='{output_directory}' \\\n",
        "    --trained_checkpoint_prefix='{last_model_path}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6paW1ut9OPC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!tflite_convert --output_file model_sdd.tflite --saved_model_dir '{last_model_path}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2rm_rnf8xaB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!ls -lh '{output_directory}/checkpoint/checkpoint'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K-kWCXH8V5-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#!rm -r '{output_directory}/saved_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sldML7iO6RVB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# # TEST\n",
        "# !echo exporting the model\n",
        "# # there is an \"Incomplete shape\" message.  but we can safely ignore that. \n",
        "# !python /content/models/research/object_detection/export_tflite_graph_lib_tf2.py \\\n",
        "#     --pipeline_config_path={pipeline_fname} \\\n",
        "#     --output_directory='{output_directory}' \\\n",
        "#     --trained_checkpoint_dir='{model_dir}'\n",
        "\n",
        "# !echo convert saved model to TF-Lite\n",
        "# #https://www.tensorflow.org/lite/models/convert/convert_models#convert_a_savedmodel_recommended_\n",
        "# # Convert the model\n",
        "# converter = tf.lite.TFLiteConverter.from_saved_model(f'{output_directory}/saved_model/') # path to the SavedModel directory\n",
        "# tflite_model = converter.convert()\n",
        "\n",
        "# # Save the model.\n",
        "# with open(f'{output_directory}/saved_model/model_ssd.tflite', 'wb') as f:\n",
        "#   f.write(tflite_model)\n",
        "# !cp {label_map_pbtxt_fname} '{output_directory}/saved_model/.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiXsVoPHEBhP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# WORKS BUT GIVES WIERD RESULT\n",
        "!echo exporting the model\n",
        "# there is an \"Incomplete shape\" message.  but we can safely ignore that. \n",
        "!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory='{output_directory}' \\\n",
        "    --trained_checkpoint_dir='{model_dir}'\n",
        "\n",
        "# !echo convert saved model to TF-Lite\n",
        "# #https://www.tensorflow.org/lite/models/convert/convert_models#convert_a_savedmodel_recommended_\n",
        "# # Convert the model\n",
        "# converter = tf.lite.TFLiteConverter.from_saved_model(f'{output_directory}/saved_model/') # path to the SavedModel directory\n",
        "# tflite_model = converter.convert()\n",
        "\n",
        "# # Save the model.\n",
        "# with open(f'{output_directory}/saved_model/model.tflite', 'wb') as f:\n",
        "#   f.write(tflite_model)\n",
        "# !cp {label_map_pbtxt_fname} '{output_directory}/saved_model/.'\n",
        "\n",
        "!tflite_convert --output_file '{output_directory}/saved_model/model_sdd.tflite' --saved_model_dir '{output_directory}/saved_model/'\n",
        "!ls -lh '{output_directory}/saved_model/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B07s1rs98-UU",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vbMEI6tI7H6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbIArXdHjzG_"
      },
      "source": [
        "To convert model see https://colab.research.google.com/github/google-coral/tutorials/blob/master/compile_for_edgetpu.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg318wYAjyJn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tensorflow_traffic_sign_detection.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}